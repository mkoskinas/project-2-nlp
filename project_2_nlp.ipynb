{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN8mKD3KxrwY4o8PeUQrRXM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkoskinas/project-2-nlp/blob/main/project_2_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "IiYrjrmH_iYn",
        "outputId": "de2107e3-1665-49af-afca-0c9ee78d18d2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>.container { width:100% !important; }</style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Read and explore the data"
      ],
      "metadata": {
        "id": "ZWo6l77UVdQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n"
      ],
      "metadata": {
        "id": "H0ouueUf_sgJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Read Data for the Twitter Fake News Challenge\n",
        "\n",
        "data = pd.read_csv(\"data/training_data_lowercase.csv\", encoding='latin-1', delimiter='\\t', names=['label', 'tweet'], skiprows=1)\n",
        "print(data.head())\n",
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OC8cF0G_2b1",
        "outputId": "e2ddc93a-37a8-4e5f-a796-d34020126753"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label                                              tweet\n",
            "0      0  drunk bragging trump staffer started russian c...\n",
            "1      0  sheriff david clarke becomes an internet joke ...\n",
            "2      0  trump is so obsessed he even has obamaâs nam...\n",
            "3      0  pope francis just called out donald trump duri...\n",
            "4      0  racist alabama cops brutalize black boy while ...\n",
            "(34151, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample a few tweets for inspection\n",
        "sample_tweets = data['tweet'].sample(10)\n",
        "\n",
        "# Print the length and full content of the sampled tweets\n",
        "for tweet in sample_tweets:\n",
        "    print(f\"Length: {len(tweet)}, Content: {tweet}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G1QE-T4Ho9j",
        "outputId": "28d1deea-6ebc-46d1-b85f-281196365b2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 58, Content: u.s. makes lower trade deficit top priority in nafta talks\n",
            "Length: 67, Content: justice department names new acting head of drug enforcement agency\n",
            "Length: 46, Content: cbo posts review of republican healthcare plan\n",
            "Length: 86, Content: fbi and doj refusing to comply with subpoena over trump dossierâhello jeff sessions?\n",
            "Length: 72, Content: house dem wants gop on record: stop govât spending at trump properties\n",
            "Length: 79, Content: sister of ny attack suspect says he may have been brainwashed; appeals to trump\n",
            "Length: 85, Content: cambodia pm hun sen says 2018 election result does not need international recognition\n",
            "Length: 80, Content: judges uphold bosnian croat convictions in last verdict of yugoslav war tribunal\n",
            "Length: 83, Content: the purge: nyc mayor de blasio to review âall symbols of hateâ on city property\n",
            "Length: 58, Content: house speaker tells trump healthcare bill lacks votes: cnn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display class distribution\n",
        "print(\"Class Distribution:\")\n",
        "print(data['label'].value_counts(normalize=True))\n",
        "\n",
        "# Basic dataset info\n",
        "print(\"\\nDataset Info:\")\n",
        "print(data.info())\n",
        "\n",
        "# Check unique values in label column\n",
        "print(\"\\nUnique Labels:\")\n",
        "print(data['label'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2_I81_MBoZm",
        "outputId": "c3ddd837-6eb7-4ebd-8b5b-ad8c9be2e9ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution:\n",
            "label\n",
            "0    0.514509\n",
            "1    0.485491\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 34151 entries, 0 to 34150\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   label   34151 non-null  int64 \n",
            " 1   tweet   34151 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 533.7+ KB\n",
            "None\n",
            "\n",
            "Unique Labels:\n",
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Divide the dataset training, validation, and test set\n",
        "\n"
      ],
      "metadata": {
        "id": "jLOwmkVrUQF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First split: Create a holdout test set (20% of data)\n",
        "train_val_data, test_data = train_test_split(\n",
        "    data,\n",
        "    test_size=0.2,  # 20% for final testing\n",
        "    random_state=42,  # for reproducibility\n",
        "    stratify=data['label']  # ensure balanced classes\n",
        ")\n",
        "\n",
        "# Second split: Create validation set from remaining data\n",
        "train_data, val_data = train_test_split(\n",
        "    train_val_data,\n",
        "    test_size=0.2,  # 20% of remaining data (16% of original data)\n",
        "    random_state=42,\n",
        "    stratify=train_val_data['label']\n",
        ")\n",
        "\n",
        "# Print split sizes\n",
        "print(f\"Total samples: {len(data)}\")\n",
        "print(f\"Training samples: {len(train_data)} ({len(train_data)/len(data):.1%})\")\n",
        "print(f\"Validation samples: {len(val_data)} ({len(val_data)/len(data):.1%})\")\n",
        "print(f\"Test samples: {len(test_data)} ({len(test_data)/len(data):.1%})\")\n",
        "\n",
        "# Save splits separately to prevent data leakage\n",
        "train_data.to_csv('data/train_data.csv', index=False)\n",
        "val_data.to_csv('data/validation_data.csv', index=False)\n",
        "test_data.to_csv('data/test_data.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PswAHS_MvW0",
        "outputId": "3368bc0e-b6cb-4678-e362-3bd94432fa01"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 34151\n",
            "Training samples: 21856 (64.0%)\n",
            "Validation samples: 5464 (16.0%)\n",
            "Test samples: 6831 (20.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the 'tweet' column\n",
        "num_missing_tweets = val_data['tweet'].isnull().sum()\n",
        "\n",
        "# Print the result\n",
        "print(f\"Number of missing tweets: {num_missing_tweets}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kdEibbbPzLV",
        "outputId": "de57dbe2-effe-47bf-8802-8524d3bb780e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing tweets: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data preprocessing"
      ],
      "metadata": {
        "id": "4VQ5KY8HWu3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HTML Content Analysis\n",
        "def check_html_presence(data, text_column='tweet'):\n",
        "    \"\"\"\n",
        "    Analyzes presence of HTML content in the dataset\n",
        "    \"\"\"\n",
        "    # Initialize counters\n",
        "    html_stats = {\n",
        "        'total_tweets': len(data),\n",
        "        'contains_html_tags': 0,\n",
        "        'contains_scripts': 0,\n",
        "        'contains_styles': 0,\n",
        "        'contains_comments': 0\n",
        "    }\n",
        "\n",
        "    # Sample tweets with HTML for inspection\n",
        "    html_examples = []\n",
        "\n",
        "    for text in data[text_column]:\n",
        "        # Check for HTML tags\n",
        "        if re.search(r'<[^>]+>', str(text)):\n",
        "            html_stats['contains_html_tags'] += 1\n",
        "\n",
        "            # Check for specific elements\n",
        "            if re.search(r'<script[^>]*>', str(text)):\n",
        "                html_stats['contains_scripts'] += 1\n",
        "            if re.search(r'<style[^>]*>', str(text)):\n",
        "                html_stats['contains_styles'] += 1\n",
        "            if re.search(r'<!--', str(text)):\n",
        "                html_stats['contains_comments'] += 1\n",
        "\n",
        "            # Store example if it's one of first 5 found\n",
        "            if len(html_examples) < 5:\n",
        "                html_examples.append(text)\n",
        "\n",
        "    # Calculate percentages\n",
        "    for key in ['contains_html_tags', 'contains_scripts', 'contains_styles', 'contains_comments']:\n",
        "        percentage = (html_stats[key] / html_stats['total_tweets']) * 100\n",
        "        html_stats[f'{key}_percentage'] = f\"{percentage:.2f}%\"\n",
        "\n",
        "    return {\n",
        "        'stats': html_stats,\n",
        "        'examples': html_examples\n",
        "    }\n",
        "\n",
        "# Run the analysis\n",
        "html_analysis = check_html_presence(data)\n",
        "\n",
        "# Print results\n",
        "print(\"HTML Content Analysis:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Total tweets analyzed: {html_analysis['stats']['total_tweets']}\")\n",
        "print(f\"Tweets containing HTML tags: {html_analysis['stats']['contains_html_tags']} ({html_analysis['stats']['contains_html_tags_percentage']})\")\n",
        "print(f\"Tweets containing scripts: {html_analysis['stats']['contains_scripts']} ({html_analysis['stats']['contains_scripts_percentage']})\")\n",
        "print(f\"Tweets containing styles: {html_analysis['stats']['contains_styles']} ({html_analysis['stats']['contains_styles_percentage']})\")\n",
        "print(f\"Tweets containing comments: {html_analysis['stats']['contains_comments']} ({html_analysis['stats']['contains_comments_percentage']})\")\n",
        "\n",
        "if html_analysis['examples']:\n",
        "    print(\"\\nExample tweets containing HTML:\")\n",
        "    for i, example in enumerate(html_analysis['examples'], 1):\n",
        "        print(f\"\\nExample {i}:\")\n",
        "        print(example[:200] + \"...\" if len(example) > 200 else example)\n",
        "else:\n",
        "    print(\"\\nNo HTML content found in tweets\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPJ57Dlna6v4",
        "outputId": "3b696d73-d43e-49b4-991b-b7175d0746cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTML Content Analysis:\n",
            "--------------------------------------------------\n",
            "Total tweets analyzed: 34151\n",
            "Tweets containing HTML tags: 0 (0.00%)\n",
            "Tweets containing scripts: 0 (0.00%)\n",
            "Tweets containing styles: 0 (0.00%)\n",
            "Tweets containing comments: 0 (0.00%)\n",
            "\n",
            "No HTML content found in tweets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tweet Elements Analysis\n",
        "def analyze_tweet_elements(data, text_column='tweet'):\n",
        "    \"\"\"\n",
        "    Analyzes presence of Twitter-specific elements in the dataset\n",
        "    \"\"\"\n",
        "    stats = {\n",
        "        'total_tweets': len(data),\n",
        "        'contains_mentions': 0,\n",
        "        'contains_hashtags': 0,\n",
        "        'contains_urls': 0,\n",
        "        'contains_rt': 0,\n",
        "        'contains_emojis': 0,\n",
        "        'contains_numbers': 0\n",
        "    }\n",
        "\n",
        "    # Store examples for each element\n",
        "    examples = {k: [] for k in stats.keys() if k != 'total_tweets'}\n",
        "\n",
        "    for text in data[text_column]:\n",
        "        text = str(text)  # Ensure text is string\n",
        "\n",
        "        # Check for mentions (@)\n",
        "        if re.search(r'@\\w+', text):\n",
        "            stats['contains_mentions'] += 1\n",
        "            if len(examples['contains_mentions']) < 3:\n",
        "                examples['contains_mentions'].append(text)\n",
        "\n",
        "        # Check for hashtags (#)\n",
        "        if re.search(r'#\\w+', text):\n",
        "            stats['contains_hashtags'] += 1\n",
        "            if len(examples['contains_hashtags']) < 3:\n",
        "                examples['contains_hashtags'].append(text)\n",
        "\n",
        "        # Check for URLs\n",
        "        if re.search(r'http\\S+|www\\S+|https\\S+', text):\n",
        "            stats['contains_urls'] += 1\n",
        "            if len(examples['contains_urls']) < 3:\n",
        "                examples['contains_urls'].append(text)\n",
        "\n",
        "        # Check for retweets\n",
        "        if re.search(r'rt @\\w+:', text, re.IGNORECASE):\n",
        "            stats['contains_rt'] += 1\n",
        "            if len(examples['contains_rt']) < 3:\n",
        "                examples['contains_rt'].append(text)\n",
        "\n",
        "        # Check for emojis (basic check)\n",
        "        if re.search(r'[^\\w\\s.,!?\\'\\\"@]', text):\n",
        "            stats['contains_emojis'] += 1\n",
        "            if len(examples['contains_emojis']) < 3:\n",
        "                examples['contains_emojis'].append(text)\n",
        "\n",
        "        # Check for numbers\n",
        "        if re.search(r'\\d+', text):\n",
        "            stats['contains_numbers'] += 1\n",
        "            if len(examples['contains_numbers']) < 3:\n",
        "                examples['contains_numbers'].append(text)\n",
        "\n",
        "    # Calculate percentages - Iterate over a copy of keys or use list comprehension\n",
        "    # This avoids modifying the dictionary during iteration\n",
        "    for key in list(stats.keys()):  # Or: for key in stats:\n",
        "        if key != 'total_tweets':\n",
        "            percentage = (stats[key] / stats['total_tweets']) * 100\n",
        "            stats[f'{key}_percentage'] = f\"{percentage:.2f}%\"\n",
        "\n",
        "    return {'stats': stats, 'examples': examples}\n",
        "\n",
        "# Run the analysis\n",
        "tweet_analysis = analyze_tweet_elements(data)\n",
        "\n",
        "# Print results\n",
        "print(\"Tweet Elements Analysis:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Total tweets analyzed: {tweet_analysis['stats']['total_tweets']}\")\n",
        "for key in tweet_analysis['stats'].keys():\n",
        "    if key != 'total_tweets' and not key.endswith('_percentage'):\n",
        "        print(f\"\\n{key.replace('_', ' ').title()}: {tweet_analysis['stats'][key]} ({tweet_analysis['stats'][key + '_percentage']})\")\n",
        "        if tweet_analysis['examples'][key]:\n",
        "            print(\"Examples:\")\n",
        "            for i, example in enumerate(tweet_analysis['examples'][key], 1):\n",
        "                print(f\"{i}. {example}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTlxW-mjdEUK",
        "outputId": "aa1fd9f0-a087-4b9f-d9d6-12fa818fff58"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet Elements Analysis:\n",
            "--------------------------------------------------\n",
            "Total tweets analyzed: 34151\n",
            "\n",
            "Contains Mentions: 6 (0.02%)\n",
            "Examples:\n",
            "1. christian âprophetâ literally loses his @ss when he takes on wild lions for jesus\n",
            "2. @ammon_bundyâs ridiculous late night twitter rant has everyone talking (tweets)\n",
            "3. american workers scr@wed over by outsourcing jobs get their day in court\n",
            "\n",
            "Contains Hashtags: 253 (0.74%)\n",
            "Examples:\n",
            "1. protesters welcome trump home to his golden tower with the best #resistance display yet (image)\n",
            "2. #trumpchicken is now trending and these tweets are hilarious (images)\n",
            "3. #bringbackobama hashtag blows up on twitter as americans share memories (tweets)\n",
            "\n",
            "Contains Urls: 7 (0.02%)\n",
            "Examples:\n",
            "1. https://100percentfedup.com/served-roy-moore-vietnamletter-veteran-sets-record-straight-honorable-decent-respectable-patriotic-commander-soldier/\n",
            "2. https://100percentfedup.com/video-hillary-asked-about-trump-i-just-want-to-eat-some-pie/\n",
            "3. https://100percentfedup.com/12-yr-old-black-conservative-whose-video-to-obama-went-viral-do-you-really-love-america-receives-death-threats-from-left/\n",
            "\n",
            "Contains Rt: 0 (0.00%)\n",
            "\n",
            "Contains Emojis: 19629 (57.48%)\n",
            "Examples:\n",
            "1. sheriff david clarke becomes an internet joke for threatening to poke people âin the eyeâ\n",
            "2. trump is so obsessed he even has obamaâs name coded into his website (images)\n",
            "3. racist alabama cops brutalize black boy while he is in handcuffs (graphic images)\n",
            "\n",
            "Contains Numbers: 3023 (8.85%)\n",
            "Examples:\n",
            "1. bad news for trump âÃ® mitch mcconnell says no to repealing obamacare in 2018\n",
            "2. cnn calls it: a democrat will represent alabama in the senate for the first time in 25 years\n",
            "3. john mccain wanted another 74 twitter followers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the necessary cleaning based on the analysis\n",
        "\n",
        "def clean_tweet(text):\n",
        "    \"\"\"\n",
        "    Optimized cleaning function that:\n",
        "    - Removes emojis and special characters\n",
        "    - Preserves numbers (for fact checking)\n",
        "    - Preserves basic punctuation\n",
        "    - Handles hashtags\n",
        "    \"\"\"\n",
        "    text = str(text)\n",
        "\n",
        "    # Remove emojis and special characters but keep numbers and basic punctuation\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\\\"@]', ' ', text)\n",
        "\n",
        "    # Handle hashtags\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Test the function with examples from the dataset\n",
        "test_cases = [\n",
        "    \"sheriff david clarke becomes an internet joke for threatening to poke people âin the eyeâ\",\n",
        "    \"cnn calls it: a democrat will represent alabama in the senate for the first time in 25 years\",\n",
        "    \"john mccain wanted another 74 twitter followers\"\n",
        "]\n",
        "\n",
        "print(\"Testing cleaning function:\")\n",
        "for i, test in enumerate(test_cases, 1):\n",
        "    print(f\"\\nTest {i}:\")\n",
        "    print(\"Original:\", test)\n",
        "    print(\"Cleaned:\", clean_tweet(test))\n",
        "\n",
        "# Apply cleaning to each split separately\n",
        "train_data['cleaned_tweet'] = train_data['tweet'].apply(clean_tweet)\n",
        "val_data['cleaned_tweet'] = val_data['tweet'].apply(clean_tweet)\n",
        "test_data['cleaned_tweet'] = test_data['tweet'].apply(clean_tweet)\n",
        "\n",
        "# 3. Save the cleaned splits\n",
        "train_data.to_csv('data/cleaned_train_data.csv', index=False)\n",
        "val_data.to_csv('data/cleaned_validation_data.csv', index=False)\n",
        "test_data.to_csv('data/cleaned_test_data.csv', index=False)\n",
        "\n",
        "# 4. Print some statistics for verification\n",
        "print(\"Dataset Split Sizes:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Training samples: {len(train_data)} ({len(train_data)/len(data):.1%})\")\n",
        "print(f\"Validation samples: {len(val_data)} ({len(val_data)/len(data):.1%})\")\n",
        "print(f\"Test samples: {len(test_data)} ({len(test_data)/len(data):.1%})\")\n",
        "\n",
        "# 5. Show examples from training set\n",
        "print(\"\\nTraining Set Cleaning Examples:\")\n",
        "print(\"-\" * 50)\n",
        "for i, (orig, cleaned) in enumerate(zip(train_data['tweet'].head(), train_data['cleaned_tweet'].head()), 1):\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    print(f\"Original: {orig}\")\n",
        "    print(f\"Cleaned:  {cleaned}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOfYO6vlexNd",
        "outputId": "3f13196b-abdb-4515-8207-17ff157d96fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing cleaning function:\n",
            "\n",
            "Test 1:\n",
            "Original: sheriff david clarke becomes an internet joke for threatening to poke people âin the eyeâ\n",
            "Cleaned: sheriff david clarke becomes an internet joke for threatening to poke people in the eye\n",
            "\n",
            "Test 2:\n",
            "Original: cnn calls it: a democrat will represent alabama in the senate for the first time in 25 years\n",
            "Cleaned: cnn calls it a democrat will represent alabama in the senate for the first time in 25 years\n",
            "\n",
            "Test 3:\n",
            "Original: john mccain wanted another 74 twitter followers\n",
            "Cleaned: john mccain wanted another 74 twitter followers\n",
            "Dataset Split Sizes:\n",
            "--------------------------------------------------\n",
            "Training samples: 21856 (64.0%)\n",
            "Validation samples: 5464 (16.0%)\n",
            "Test samples: 6831 (20.0%)\n",
            "\n",
            "Training Set Cleaning Examples:\n",
            "--------------------------------------------------\n",
            "\n",
            "Example 1:\n",
            "Original: bombshell report: nsa offered to give hillaryâs emails to fbiâjames comey rejected them\n",
            "Cleaned:  bombshell report nsa offered to give hillary s emails to fbi james comey rejected them\n",
            "\n",
            "Example 2:\n",
            "Original: factbox: where chile's top presidential candidates stand on reforms\n",
            "Cleaned:  factbox where chile's top presidential candidates stand on reforms\n",
            "\n",
            "Example 3:\n",
            "Original: obama: trump doesn't know much about foreign policy\tworld\n",
            "Cleaned:  obama trump doesn't know much about foreign policy world\n",
            "\n",
            "Example 4:\n",
            "Original: trump criticizes billions in spending on u.s. air traffic control\n",
            "Cleaned:  trump criticizes billions in spending on u.s. air traffic control\n",
            "\n",
            "Example 5:\n",
            "Original: fox news host destroys his colleagues for kissing donald trumpâs ring (tweets)\n",
            "Cleaned:  fox news host destroys his colleagues for kissing donald trump s ring tweets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "import nltk\n",
        "from sklearn.metrics import classification_report\n",
        "import re\n",
        "\n",
        "# 1. Load the data\n",
        "train_data = pd.read_csv('data/cleaned_train_data.csv')\n",
        "val_data = pd.read_csv('data/cleaned_validation_data.csv')\n",
        "\n",
        "# 2. Create Model Pipeline\n",
        "def create_fake_news_detector():\n",
        "    # Use BERT model specifically fine-tuned for fake news\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    # Create text preprocessing layers\n",
        "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "    preprocessed_text = tf.keras.layers.Lambda(clean_tweet)(text_input)\n",
        "\n",
        "    # Create tokenizer layer\n",
        "    encoder = tokenizer(\n",
        "        preprocessed_text,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    # BERT outputs\n",
        "    outputs = model(encoder)\n",
        "\n",
        "    # Add custom layers on top of BERT\n",
        "    x = tf.keras.layers.Dense(64, activation='relu')(outputs.logits)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Create final model\n",
        "    model = tf.keras.Model(inputs=[text_input], outputs=outputs)\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# 3. Training Configuration\n",
        "def train_model(model, X_train, y_train, X_val, y_val):\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC()]\n",
        "    )\n",
        "\n",
        "    # Add callbacks\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=3,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.2,\n",
        "            patience=2\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=10,\n",
        "        batch_size=16,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "# 4. Evaluation and Analysis\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    # Get predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    # Print classification report\n",
        "    print(classification_report(y_test, y_pred_classes))\n",
        "\n",
        "    # Analyze errors\n",
        "    errors = pd.DataFrame({\n",
        "        'text': X_test[y_test != y_pred_classes],\n",
        "        'true_label': y_test[y_test != y_pred_classes],\n",
        "        'predicted_label': y_pred_classes[y_test != y_pred_classes],\n",
        "        'confidence': y_pred[y_test != y_pred_classes]\n",
        "    })\n",
        "\n",
        "    return errors\n",
        "\n",
        "# 5. Feature Importance Analysis\n",
        "def analyze_feature_importance(model, tokenizer, text):\n",
        "    # Use SHAP values or attention weights\n",
        "    from transformers import pipeline\n",
        "    explainer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "    # Get token attributions\n",
        "    attribution = explainer(text, return_tensors=True)\n",
        "\n",
        "    return attribution"
      ],
      "metadata": {
        "id": "8y6WJCZtWwJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_for_bert(text):\n",
        "    \"\"\"\n",
        "    Minimal preprocessing for BERT\n",
        "    \"\"\"\n",
        "    # Convert to string\n",
        "    text = str(text)\n",
        "\n",
        "    # Remove excessive whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # Remove URLs (optional, but often helpful)\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def prepare_bert_data(texts, tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Encode texts for BERT\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "# 1. Prepare the data\n",
        "print(\"Preparing data...\")\n",
        "train_data['processed_text'] = train_data['text'].apply(preprocess_for_bert)\n",
        "val_data['processed_text'] = val_data['text'].apply(preprocess_for_bert)\n",
        "test_data['processed_text'] = test_data['text'].apply(preprocess_for_bert)\n",
        "\n",
        "# 2. Initialize BERT tokenizer and model\n",
        "print(\"Initializing BERT...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "\n",
        "# 3. Encode data\n",
        "print(\"Encoding data...\")\n",
        "train_encodings = prepare_bert_data(train_data['processed_text'].tolist(), tokenizer)\n",
        "val_encodings = prepare_bert_data(val_data['processed_text'].tolist(), tokenizer)\n",
        "test_encodings = prepare_bert_data(test_data['processed_text'].tolist(), tokenizer)\n",
        "\n",
        "# 4. Prepare labels\n",
        "y_train = train_data['label'].values\n",
        "y_val = val_data['label'].values\n",
        "y_test = test_data['label'].values\n",
        "\n",
        "# 5. Configure training parameters\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# 6. Compile model\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "# 7. Setup callbacks\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=2,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        'best_bert_model.h5',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# 8. Train model\n",
        "print(\"Training model...\")\n",
        "history = model.fit(\n",
        "    train_encodings,\n",
        "    y_train,\n",
        "    validation_data=(val_encodings, y_val),\n",
        "    epochs=3,  # Start with small number of epochs\n",
        "    batch_size=16,  # Smaller batch size for better generalization\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# 9. Evaluate model\n",
        "print(\"\\nEvaluating model...\")\n",
        "test_results = model.evaluate(test_encodings, y_test)\n",
        "print(f\"Test accuracy: {test_results[1]:.4f}\")\n",
        "\n",
        "# 10. Save model and tokenizer\n",
        "print(\"\\nSaving model and tokenizer...\")\n",
        "model.save_pretrained('fake_news_bert_model')\n",
        "tokenizer.save_pretrained('fake_news_bert_tokenizer')\n",
        "\n",
        "# 11. Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 12. Function for making predictions\n",
        "def predict_fake_news(text, model=model, tokenizer=tokenizer):\n",
        "    \"\"\"\n",
        "    Make prediction on new text\n",
        "    \"\"\"\n",
        "    # Preprocess\n",
        "    processed_text = preprocess_for_bert(text)\n",
        "\n",
        "    # Encode\n",
        "    encoding = prepare_bert_data([processed_text], tokenizer)\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(encoding)[0]\n",
        "    probability = tf.sigmoid(prediction).numpy()[0]\n",
        "\n",
        "    return {\n",
        "        'text': text,\n",
        "        'probability_fake': float(probability),\n",
        "        'prediction': 'FAKE' if probability > 0.5 else 'REAL'\n",
        "    }\n",
        "\n",
        "# Test prediction function\n",
        "sample_texts = [\n",
        "    \"Breaking: Scientists discover miracle cure for all diseases!\",\n",
        "    \"New study shows correlation between exercise and health benefits\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting predictions:\")\n",
        "for text in sample_texts:\n",
        "    result = predict_fake_news(text)\n",
        "    print(f\"\\nText: {result['text']}\")\n",
        "    print(f\"Prediction: {result['prediction']}\")\n",
        "    print(f\"Probability of being fake: {result['probability_fake']:.2%}\")"
      ],
      "metadata": {
        "id": "3ZO4n8Xusoq7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}